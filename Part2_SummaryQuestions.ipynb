{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 2: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**   \n",
    "The receptive field is the area in the input space which produces the features, the receptive fields of different features partially overlap and as such cover the entire input space. \n",
    "When stacking convolutional layers, the receptive fields combine and each feature takes input from a larger area of pixels in the previous layer image. When the receptive field grows, while progressing through the convolutional layers, it takes into account the value of a pixel and it's surrounding pixels (the closer to the center a pixel is, the higher it's value is).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Three different ways to control the rate at which the receptive field grows:\n",
    "\n",
    "\n",
    "1. Pooling:  \n",
    "The pooling layer is a way to reduce the dimension of the feature map, by combining features in the same region. Thus, the successive conv layers are affected by increasingly larger parts of the input image, which results in a rapid increase in the receptive field size.  \n",
    "\n",
    "2. Stride:  \n",
    "Stride determines the shift size of the filter, in other words it detrmines how big is the overlapping portion of the receptive fields between features. Therefore, the larger the stride is, the smaller the overlapping portion of pixels are between features, the larger the receptive filed will grow between layers.\n",
    "\n",
    "3. Dilation:   \n",
    "Dilation determines the spacing between pixels in the filter frame. Using delation grows the receptive field exponentialy between layers without effecting the paramteres, which will continue to grow linearly. \n",
    "\n",
    "Pooling combines feaures in the same region, which makes the model more robust to variations in the position of the input features. Unlike Stride, which in each conv layer combines the features while producing the feature map. Finally, with Dilation the weights are matched to distant pixels in the input image, the distance in determind by delation parameter, and so this results in a more globel context of the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 122, 122])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Each receptive field size is derived from the layer before it, hence we can calculate the network receptive field recursively. For convenience let's look at the relevant data for each layer in the table below:  \n",
    "\n",
    "<table style=\"width:60%\">\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th> layer 1 </th>\n",
    "        <th> layer 2 </th>\n",
    "        <th> layer 3 </th>\n",
    "        <th> layer 4 </th>\n",
    "        <th> layer 5 </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> k </th>\n",
    "        <td> 3 </td>\n",
    "        <td> 2 </td>\n",
    "        <td> 5 </td>\n",
    "        <td> 2 </td>\n",
    "        <td> 7 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> s </th>\n",
    "        <td> 1 </td>\n",
    "        <td> 2 </td>\n",
    "        <td> 2, 4 </td>\n",
    "        <td> 2, 8 </td>\n",
    "        <td> 1, 8 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> d </th>\n",
    "        <td> 1 </td>\n",
    "        <td> 1 </td>\n",
    "        <td> 1 </td>\n",
    "        <td> 1 </td>\n",
    "        <td> 2 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "k = kernel, s = stride (layer stride, cumulative stride), d = diliation\n",
    "Layer 2 and 4 are the max pooling layers, we'll treat them as if they were conv layer with k = 2 and s = 2. \n",
    "\n",
    "The recursive formula for the receptive field size of the output tensor: $ r_0 = \\sum_{i=1}^L (d_i(k_i - 1) \\cdot \\Pi_{j=0}^{j-1} s_j) + 1 $\n",
    "\n",
    "In the $ i $ iteration we calculate the receptive field of the $ i $ layer, which comes to be the sum of the previous layers receptive fields and the accumaltive stride. \n",
    "For example the first layer, the receptive field will be 3, since it's only affected by the kernel size. The formula produces the same result: $ r_1 = 1 \\cdot (k_1 - 1) + 1 = (3 - 1) + 1 = 3 $.   \n",
    "Now lets caclulate the outputs receptive field: \n",
    "$ r_0 = (1 \\cdot (3 - 1) \\cdot 1) +  \n",
    "(1 \\cdot (2 - 1) \\cdot 1) + \n",
    "(1 \\cdot (5 - 1) \\cdot 1 \\cdot 2) +\n",
    "(1 \\cdot (2 - 1) \\cdot 1 \\cdot 2 \\cdot 2) + \n",
    "(2 \\cdot (7 - 1) \\cdot 1 \\cdot 2 \\cdot 2 \\cdot 2) + 1 = 112 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "The layers before this change were learning the true output - $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$.\n",
    "By changing to residual mapping, the residual block now learns the \"residual mapping\" - $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$. If we rearange this expression we get - $ f_l(\\vec{x};\\vec{\\theta}_l) = \\vec{y}_l - \\vec{x} $, there is a an identity connection due to x, therefore the layers are trying to learn the delta. As a result the network produces different filters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:17.992615Z",
     "iopub.status.busy": "2021-01-26T09:18:17.991868Z",
     "iopub.status.idle": "2021-01-26T09:18:18.013482Z",
     "shell.execute_reply": "2021-01-26T09:18:18.014164Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "The Dropout layer with parameter p will 'zero' an element with a probability of p, or keep the element with a probability of $(1-p)$.  \n",
    "In order to combine the dropout layers, will examine the probability for element X to be dropped out. The probability to drop the X element in the first dropout layer is $ p_1 $. The probability to drop the X element in the second dropout layer, is the probability not to drop it in the first layer- $ (1 - p_1) $ multiplyed by the probability to drop it in the second layer $ p_2 $.   \n",
    "Finally we get: $ q = p_1 + (1 - p_1) \\cdot p_2 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**   \n",
    "False.\n",
    "Common parctice is to apply Dropout after non-linear activation functions. However, when using rectified linear units, such as ReLU, the decision for placing the Dropout layer before or after the activation layer will depend on the perticular code implementation due to computational efficiency. \n",
    "Let's demonstrate that for simple examples there is no difference:\n",
    "\n",
    "Network A: fully connected, linear activation -> ReLU -> Dropout -> ...  \n",
    "Network B: fully connected, linear activation -> Dropout -> ReLU -> ...\n",
    "\n",
    "Let the output of the linear activation be: $[-6, -7, -8, 9, 1, 2]$.  \n",
    "For both networks will use Dropout with $p = 0.5$, and will assume that the elements that are being zeroed are 2, 4, 6.  \n",
    "\n",
    "Let us pass it through A first:  \n",
    "ReLU($[-6, -7, -8, 9, 1, 2]$) = $[0, 0, 0, 9, 1, 2]$   \n",
    "Dropout($[0, 0, 0, 9, 1, 2]$) = $[2*0, 0, 2*0, 0, 2*1, 0] = [0, 0, 0, 0, 2, 0] $\n",
    "\n",
    "Now through B:   \n",
    "Dropout($[-6, -7, -8, 9, 1, 2]$) =$[2*(-6), 0, 2*(-8), 0, 2*1, 0] = [-12, 0, -16, 0, 2, 0]$   \n",
    "ReLU($[-12, 0, -16, 0, 2, 0]$) = $[0, 0, 0, 0, 2, 0]$  \n",
    "\n",
    "As excpected, we got the same output from both networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Dropout with parameter $p$ will drop the input elements with $p$ probability, and keep the elements with probability $(1-p)$. \n",
    "Therefore, the elements who 'survive' the dropout are multiplied by $(1-p)$ (as we saw in the example above). In order to maintain the value of each activation unchanged we need to undo the multipcation added by the dropout. Therefore we must scale by $1/ (1-p)$ to undo this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "We would not train a binary classification model with the L2 loss function (MSE), a more fitting loss function is the Cross-Entropy loss function.  \n",
    "The reason for this is that the MSE function expects real-value inputs in range $(- \\infty, \\infty)$, while binary classification modles output probabilities in \n",
    "range $(0, 1)$ through the sigmoid/ logistic function.   \n",
    "Let's demonstarte this idea with a graph: \n",
    "\n",
    "<center><img src=\"losses q1.png\" width=\"1000\" height=\"1000\" /></center>\n",
    "\n",
    "The graph displays the MSE (blue) with the sigmoid activation, to normalize the output to fit the range $(0,1)$, and the cross-entropy (red) also with the sigmoid activation.  \n",
    "It can be seen in the graph, that for example when the output is $-10$, the MSE has no slope, therefore the gradient will get stuck, and the model or will train very slowly or won't be able to train. On the other hand the cross entropy has a slope and therefore the model will be able to train.   \n",
    "Direct calculation of the gradient produces the following result: \n",
    "\n",
    "$$ L_2 = (1- {{1} \\over {1 + e^-x}})^2 \\Longrightarrow {{\\partial L_2} \\over {\\partial \\mat{x}}} = {{\\mat{e}^{-\\mat{x}} log(\\mat{e})} \\over {(1 + \\mat{e}^{-\\mat{x}})^2}} $$\n",
    "\n",
    "For $\\ \\mat{x} = -10 \\ $ the dervitive value is:    $ \\ \\ {{\\mat{e}^{10} log(\\mat{e})} \\over {(1 + \\mat{e}^{10})^2}} = 1.971e-5 \\sim 0 $  \n",
    "The result is as expected from the graph, numericaly this result is equivelnt to zero, and that is way $L_2$ isn't recommended for a binary classification model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.019108Z",
     "iopub.status.busy": "2021-01-26T09:18:18.018447Z",
     "iopub.status.idle": "2021-01-26T09:18:18.042969Z",
     "shell.execute_reply": "2021-01-26T09:18:18.043669Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "The model that is described above is very deep, and is only using sigmoid as an activation function. The `sigmoid` function has a single output bounded by $[0.0, 1.0]$, that saturates when its input is extremely negative or extremely positive. Therefore, it's most likly the model is suffering from a vanishing gradient, which causes the saturation in the `sigmoid` function and thus, seen as a plateau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Replacing the `sigmoid` activations with `tanh` probably wont't help, because tanh suffers from vanishing gradient as well. The `tanh` and `sigmoid` activations are very similar, the difference between them is range, `tanh`'s range is $[-1.0, 1.0]$ as opposed to `sigmoid`'s range, which is $[0.0, 1.0]$. Therefore, if the model reached a plateau with `sigmoid` it's most likely to behave the same with `tanh`. It seems that in this case, the best course of action would be to go with ReLU activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "  1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "  1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "  1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anaswer:**  \n",
    "A. **False**. ReLU suffers less from vanishing gradient, but it does saturate when the input is extremely negative. \n",
    "\n",
    "B. **False**. The ReLU activation function is defined as $f(x) = max\\{0, x\\}$. If $x$, the input, is positive then $f(x) = x$, and we get the identity function.   \n",
    "The gradient of the identity function is 1. Therefore, the gradient is not linear with respect to the input.  \n",
    "\n",
    "C. **True**. There is a case when large weight updates can mean that the summed input to the activation function is always negative, regardless of the input to the network.  \n",
    "It is likely that this happens when learning a large negative bias term for its weights.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "*  GD - \n",
    "    * Uses the entire data set to calculate the gradient.\n",
    "    * Slow and takes a long time to train.\n",
    "    * Usually used on small data sets.\n",
    "* SGD -\n",
    "    * Calculates the Gradient using only one random sample from the data set.\n",
    "    * Converges fast and used mostly on large data sets where the parameters update more frequently.\n",
    "    * We can't use a vectorized approach to speed up the process.\n",
    "* mini-batch SGD - \n",
    "    * Mixture of GD and SGD, uses mini-batch of samples from the data set.\n",
    "    * Will converge fast like SGD.\n",
    "    * We can use the vectorized approach on each mini-batch to speed up the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD:\n",
    "  1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "  2. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "2.1 SGD calculates the gradient of exactly one random sample from the data set and updates the parameters according to the sample gradient when GD calculates the gradient of the entire data set which will take more computation time and slows down the learning process so as a result SGD will converge a lot faster. \n",
    "On top of that SGD will result in a less over-fitted model than GD because in each iteration the parameters are updated according to one sample.\n",
    "\n",
    "2.2 GD can't be used when the loss surface has lots of local minima, GD will then get stuck in one of them and probably will not learn past that point. If the loss surface is non-convex the GD will learn very slow and can be not useful as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "It depends, Increasing the batch size in mini-batch SGD will bring the algorithm closer to GD so that the calculated gradient will be close to the minima so the number of iteration will increase because the steps the algorithm take towards convergence will be smaller, but because we now have GPU with twice the RAM we can now use vectorization with a bigger batch in each iteration so the __Time__ it will take to get to that loss value will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "  1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "  1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "  1. Training  with SGD requires more memory than with GD.\n",
    "  1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "  1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "1. True, in SGD algorithm for each epoch for each sample we, calculate its gradient and update the weights respectivly.\n",
    "1. False, SGD calculates gradients based on a single sample which will results in an error from the true gradient, so the learning graph can be very noisy and slower then GD but in practice because the time it takes to calculate a single sample gradient instead of the whole dataset is much smaller SGD convergence faster then GD when the data set is large enghoth  \n",
    "1. True, because the SGD uses only one sample each time his step are relativley large and noisy compare to the GD step so even if SGD got to a local minima the next step will probably release him.\n",
    "1. False, SGD calculates one samples gradient each time in oppose to GD that calculates the entire data set gradinet.\n",
    "1. False, there is no way to garenty that SGD or GD will converge to a global minima, there are ways to increase the chances of that to happend but they both might miss it and got stuck on a local minima.\n",
    "1. False, Assuming we have a loss surface with narrow ravine, we can approximate the loss surface to polynomial function, when doing so Newton's method will converge in  asingle iteration because it considers both gradient and curvature when for SGD with mumentum the convergence will happend after several iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In tutorial 5 we saw an example of bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "  **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "False, the outer level net dosn't depend on the inner optimization problem therfore the output of the inner net can be calculate using any method that will result in a good output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "  1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "  2. How can each of these problems be caused by increased depth?\n",
    "  3. Provide a numerical example demonstrating each.\n",
    "  4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "1. vanishing gradients - when using gradient-based algorithms and backpropagation where each weight in the network updates according to the partial derivative of the error function concerning the current weight. In some cases, the gradient will decrease slowly which will result in a very slow learning rate as the depth of the net grows\n",
    "\n",
    "exploding gradient - The same as vanishing gradients only when some activation function has a large gradient. In that case, the learning rate will be very large and can cause the model to miss its minima and keep growing and growing and ruin the learning process.\n",
    "\n",
    "2. When the number of layers of the network grows the gradients accumulate through the backpropagation process so for deep layers the change needed to be made to each weight is very small or very large depending on the gradient which can result in vanishing or exploding gradients.\n",
    "\n",
    "3. vanishing gradient - For example, the Hyperbolic tangent has a gradient in range (0,1), backpropagation computes gradients by the chain rule so for a net with *n* layers the accumulation will have the effect of multiplying those small gradients *n* times which will decrease them exponentially and can result in vanishing gradients for the early layers.\n",
    "\n",
    "exploding gradient - let's assume we have an *n* layers net where the initial weights are larger than 1, so because of the backpropagation the gradients of the loss will be multiplied by those weights and will get exponentially larger as we dive deeper into the net which will result in an exploding gradient.\n",
    "\n",
    "4. By looking at the loss graph through the learning rate we can determine which of the two problems we have, if the loss is increasing for a few iterations and keeps growing we can assume that we have exploding gradient because the model is not learning and the weights keep getting bigger and far from the ground truth.\n",
    "on the other hand, if we see that the loss value stays fixed or decreasing very slowly we can assume that we have a vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y,\\hat{y}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "We'll find the derevitives using the cahin rule as we saw in the course lectures.\n",
    "\n",
    "For convenience: $ \\mat{z} = \\mat{W}_1 x+ b_1 $\n",
    "\n",
    "$$ {{\\partial \\ell} \\over {\\partial \\hat{y}}} = {{-y} \\over \\hat{y}} + {{1-y} \\over {1 -\\hat{y}}} = {{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} $$\n",
    "\n",
    "$$ \n",
    "{{\\partial L_s} \\over {\\partial \\mat{W}_1}} = \n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial \\mat{z}}} *\n",
    "{{\\partial \\mat{z}} \\over {\\partial \\mat{W}_1}} +\n",
    "\\lambda  \\norm{\\mat{W}_1}_F = \n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} \\cdot\n",
    "\\mat{W}_2^T \\cdot \\varphi ' (\\mat{z}) \\cdot \\mat{x}^T + \\lambda  \\norm{\\mat{W}_1} _F\n",
    "$$\n",
    "\n",
    "$$\n",
    "{{\\partial L_s} \\over {\\partial \\mat{W}_2}} = \n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial \\mat{W}_2}} +\n",
    "\\lambda  \\norm{\\mat{W}_2}_F = \n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} \\cdot\n",
    "\\varphi(\\mat{z})^T + \\lambda  \\norm{\\mat{W}_2}_F\n",
    "$$\n",
    "\n",
    "$$\n",
    "{{\\partial L_s} \\over {\\partial b_1}} = \n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial \\mat{z}}} *\n",
    "{{\\partial \\mat{z}} \\over {\\partial b_1}} =\n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} \\cdot\n",
    "\\mat{W}_2^T \\cdot \\varphi '(\\mat{z})\n",
    "$$\n",
    "\n",
    "$$\n",
    "{{\\partial L_s} \\over {\\partial b_2}} =\n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial b_2}} = \n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{{\\partial L_s} \\over {\\partial \\mat{x}}} =\n",
    "{{\\partial \\ell} \\over {\\partial \\hat{y}}} * \n",
    "{{\\partial \\hat{y}} \\over {\\partial \\mat{z}}} *\n",
    "{{\\partial \\mat{z}} \\over {\\partial \\mat{x}}} = \n",
    "{1 \\over N }\\sum_{i=1}^N{{\\hat{y} - \\mat{y}} \\over {\\hat{y}(1- \\hat{y})}} \\cdot\n",
    "\\mat{W}_2^T \\cdot \\varphi ' (\\mat{z}) \\cdot \\mat{W}_1^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Derivative a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "  1. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD).\n",
    "  \n",
    "  2. What are the drawbacks of this approach? List at least two drawbacks compared to AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "\n",
    "A. Given that our loss function is $\\mat{f}(\\mat{x}) = \\mat{W} \\mat{x} + b$, by chosing a very small $\\Delta$, we could use the formula above with our loss function, during backpropgation to update our weights.  \n",
    "To get a more accurate approximation, we could select a small $\\Delta$ from each sides of the input. By doing so we limit from both sides the results of the formula, and thus, update our wehights more accuratly.\n",
    "\n",
    "\n",
    "B. Two drawbacks:   \n",
    "\n",
    "Numerical: Using the dervitive formula will give us an approximation of the result since we are using a delta, as opposed to AD which gives as an exact result. \n",
    "    \n",
    "Complexity: If we would have used this formula insead of AD, we would have to calculate $\\mat{f}(\\mat{x})$ twice, once with $\\mat{x}_0$ and the second time with $\\mat{x}_0 + \\Delta \\mat{x}$.  \n",
    "Doing so is computionaly expensive, and depends on the number of parameters in $\\mat{f}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Given the following code snippet:\n",
    "  1. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question.\n",
    "  2. Calculate the same derivatives with autograd.\n",
    "  3. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.4910, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "# TODO: Calculate gradients numerically for W and b\n",
    "grad_W = torch.zeros(W.shape, dtype=dtype)\n",
    "grad_b = torch.zeros(b.shape, dtype=dtype)\n",
    "delta = 1e-7\n",
    "W_delta = torch.clone(W)\n",
    "b_delta = torch.clone(b)\n",
    "\n",
    "for i in range(d):\n",
    "    for j in range(d):\n",
    "        W_delta[i][j] = W[i][j] + delta\n",
    "        loss_delta = foo(W_delta, b) \n",
    "        grad_W[i][j] = (loss_delta - loss) / delta\n",
    "        W_delta[i][j] = W[i][j]\n",
    "\n",
    "for i in range(d):\n",
    "    b_delta[i] = b[i] + delta\n",
    "    loss_delta = foo(W, b_delta)\n",
    "    grad_b[i] = (loss_delta - loss) / delta\n",
    "    b_delta[i] = b[i]\n",
    "\n",
    "# TODO: Compare with autograd using torch.allclose()\n",
    "loss.backward()\n",
    "autograd_W = W.grad\n",
    "autograd_b = b.grad\n",
    "\n",
    "assert torch.allclose(grad_W, autograd_W)\n",
    "assert torch.allclose(grad_b, autograd_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings:\n",
    "  1. Explain this term and why it's used in the context of a language model.\n",
    "  1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "A. Word embedding in the context of language models is the representation of words as vectors that encode the meaning of the word, so that words that are closer in the vecotr space are expected to be closer in meaning. The mapping between words that are in a space with many dimensions per word to a continuous vector space with a much lower dimension, is done by neural networks and therefore by a language model.   \n",
    "\n",
    "B. Yes, a language model could train with one-hot encoding instead of word embedding. But, there a few drawbacks to this method: \n",
    " - We loose the semantics realationship between the words in the text. In this method, each vectorization is an orthogonal representation in another dimension. This could possibly result in low preformance of the model. \n",
    " - Another disadvantage is the scale when the number of output labels is large. In language modeling the number of out put lables equals to the vacabulary size, this means that each iput feature (word) will be represented as a large vector, and most of the values in the vector will be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "  1. What does `Y` contain? why this output shape?\n",
    "  2. How you would implement `nn.Embedding` yourself using only torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "A. Embedding is the mapping between words in a high dimension space to vectors in a (much) lower dimension space. Y contains the embedding of the word set X, the words in X are of shape $[5, 6, 7, 8]$. \n",
    "The embedding_dim represents the vector size, and so, Y is a tensor that contains the mapping between words of shape $[5, 6, 7, 8]$ to vectors of length 42000, that is why Y's shape is $[5, 6, 7, 8, 42000]$.\n",
    "\n",
    "B. An impelmentation of Embedding using only tensors:   \n",
    "Initialize a tensor of shape [num_embeddings, embedding_dim] with random values, which will be the dictionary. Each word in the input, will be used as the key to locate the correspanding row in the dictionary. The rows values will be the vector representing the word. The mapping described will produce the same embedding for the same word, and the final shape of the embedding will be [the words shape, embedding_dim].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain.\n",
    "  1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "  2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S.\n",
    "  3. TBPTT allows the model to learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "A. False. TBPTT uses the backpropasgation algoritham as is. The algoritham is performed for a fixed number of timesteps, which truncates the amount of calculations and the amount of RNN outputs saved to the memory (to solve the vanishing\\ exploding weights). \n",
    "\n",
    "B.  Fasle. TBPTT limits the number of timestamps is accuumilated in the model before preforming the backpropgation algoritham. The length of the sequence doesn't play a role in this case, instead we could limit the number of timestamps to S. If we would limit the sequences length, we wouldn't address the problems TBPTT is trying to solve, since an input sequences could still be comprised of thousands of timesteps, then this will be the number of derivatives required for a single weight update. \n",
    "\n",
    "C.  True. In TBPTT the compuitaitonal graph is truncated every S timestamps. The inputs in that time frame are backpropgated on, and there is no memory of input that came before it (and obviouslly after). For input with a timestamp t, the farthest back relationship it can learn is of (S-t). Therefore, the model can learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 7 (part 2) we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "  1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the shiny new transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "A. The attention mechanism retains and utilises all the hidden states of the input sequence during the decoding process. It does this by creating a unique mapping between each time step of the decoder output to all the encoder hidden states. This means that for each output that the decoder makes, it has access to the entire input sequence and can selectively pick out specific elements from that sequence to produce the output.  \n",
    "On the other hand, the standard seq2seq model uses only the last hidden state of the encoder RNN as the context vector for the decoder, and generally is unable to accurately process long input sequences.\n",
    "\n",
    "B. Self-attention allows the model to learn to access information from the past hidden layer, at the cost of very expensive computational calculations in the decoder. \n",
    "A self-attention layer connects all positions with a constant number of sequentially\n",
    "executed operations, whereas a recurrent layer requires O(n) sequential operations. To improve computational performance for tasks involving\n",
    "very long sequences, self-attention could be restricted to considering only a neighborhood of size r. This answer is influenced by the paper \"Attention Is All You Need\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "  1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "  1. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  2. Every time we feed an image to the encoder, then decode the result, we'll get the same reconstruction.\n",
    "  3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "  2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "  3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "  5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have implemented a graph convolutional layer based on the following formula, for a graph with $N$ nodes:\n",
    "$$\n",
    "\\mat{Y}=\\varphi\\left( \\sum_{k=1}^{q} \\mat{\\Delta}^k \\mat{X} \\mat{\\alpha}_k + \\vec{b} \\right).\n",
    "$$\n",
    "  1. Assuming $\\mat{X}$ is the input feature matrix of shape $(N, M)$: what does $\\mat{Y}$ contain in it's rows?\n",
    "  1. Unfortunately, due to a bug in your calculation of the Laplacian matrix, you accidentally zeroed the row and column $i=j=5$ (assume more than 5 nodes in the graph).\n",
    "What would be the effect of this bug on the output of your layer, $\\mat{Y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We have discussed the notion of a Receptive Field in the context of a CNN. How would you define a similar concept in the context of a GCN (i.e. a model comprised of multiple graph convolutional layers)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}